# Used to label kubernetes resources. This should only differ between different
# product/brandings. It shouldn't be overriden when deploying the chart.
productShortName: mist

# Settings for docker images used in the chart.
image: &image-properties
  registry: mistce
  tag: master
  pullPolicy: Always
  secrets:
    - name: registry

# HTTP settings for the deployed application.
http:
  host: localhost
  http2: false
  static: ''
  tlsSecret: ''
  tlsHosts: []
  tlsAnnotations: {}
  tlsClusterIssuer: ''

# Email settings. Change this to connect to a real SMTP server.
smtp:
  host: ''
  port: 8025
  username: ''
  password: ''
  tls: false
  starttls: false

# Elasticsearch settings. Change this to use a remote elasticsearch cluster.
elasticsearch:
  host: ''
  port: 9200
  username: ''
  password: ''
  tls: false
  verifyCerts: false

# InfluxDB settings. Change this to use a remote InfluxDB cluster.
influxdb:
  host: ''
  port: 8086
  db: telegraf
  monitoring: true

# TSFDB settings.
tsfdb:
  #image: *image-properties
  enableSubchart: false
  nameOverride: mist

# Options for rabbitmq subchart.
rabbitmq:
  enabled: true
  auth:
    username: guest
    password: guest
    erlangCookie: guest
  metrics:
    enabled: true
  replicaCount: 1
  replicationFactor: 1
  clustering:
    forceBoot: true
  resources:
    requests:
      memory: 250Mi
      cpu: 1000m
    limits:
      memory: 500Mi
      cpu: 2000m
  service:
    clusterIP: ''
  nodeSelector: {}
  updateStrategy: RollingUpdate
  extraConfiguration: |-
    tcp_listen_options.keepalive = true
    cluster_formation.node_cleanup.only_log_warning = true
    load_definitions = /app/load_definition.json
  loadDefinition:
    enabled: true
    existingSecret: |-
      {{ .Release.Name }}-rabbitmq-definition

# Use an external rabbitmq. This will only be used
# if `rabbitmq.enabled` is false.
rabbitmqExternal:
  host: rabbitmq
  port: 5672
  username: guest
  password: guest

# MongoDB settings. Change this to use a remote MongoDB cluster.
mongodb:
  host: ''
  port: 27017
  auth:
    enabled: false

# Docker settings (used by orchestration plugin). This has to be configured for
# orchestration to work.
docker:
  host: api
  port: 2375
  key: ''
  cert: ''
  ca: ''

# Monitoring settings.
monitoring:
  defaultMethod: telegraf-influxdb

# Allow registration of dev users, used for testing purposes.
enableDevUsers: false

# Set to false to serve raw UI files. Used during development.
jsBuild: true

# Internal authentication, encryption, signing keys.
internalKeys:
  sign: CHANGEME
  secret: ''
  cilia: ''

# Authentication configuration.
auth:
  email:
    signup: true
    signin: true
  google:
    signup: false
    signin: false
    key: ''
    secret: ''
  github:
    key: ''
    secret: ''
backup:
  key: ''
  secret: ''
  bucket: ''
  gpg:
    recipient: ''
    public: ''
# Used to clone repos.
githubBotToken: ''

# Currently only used by the SaaS version.
gitlabToken: ''
slackWebhooks:
  billing: ''
mixpanelToken: ''
customerio:
  sideId: ''
  apiKey: ''
googleAnalyticsId: ''
olarkId: ''
stripe:
  publicApiKey: ''
  secretApiKey: ''
sendgridEmailNotificationsKey: ''

# The (possibly multi-line) string given here, will be appended, as is, to the
# `settings.py` file. This is a quick and dirty way of overriding options not
# directly exposed in this file.
extraSettings: ''

# Node selector will be applied to all pods for all deployments, excluding
# subcharts.
nodeSelector: {}

preemptible:
  tolerations:
  - key: "preemptible"
    operator: "Equal"
    value: "true"
    effect: "NoSchedule"
  nodeAffinity:
    # requiredDuringSchedulingIgnoredDuringExecution:
    #   nodeSelectorTerms:
    #   - matchExpressions:
    #     - key: "preemptible"
    #       operator: "In"
    #       values:
    #       - "true"
    preferredDuringSchedulingIgnoredDuringExecution:
    - weight: 1
      preference:
        matchExpressions:
        - key: "preemptible"
          operator: "In"
          values:
          - "true"

# Configure prometheus exporters. Subcharts may require separate configuration.
prometheus:
  enabled: false
  operator:
    enabled: false
    serviceMonitor:
      interval: 20s
      namespace: monitoring
      selector:
        prometheus: kube-prometheus
    rules:
      interval: 20s
      namespace: monitoring
      selector:
        prometheus: kube-prometheus

# Deployment options (replicas, resource quotas etc).
deployment:

  # Settings for celery worker deployments.
  celery:

    # Generic runners. On by default.
    prefork:
      enabled: true
      pool: prefork
      replicas: 1
      autoscaling: &autoscaling
        enabled: false
        minReplicas: 1
        maxReplicas: 20
        targetCPUUtilizationPercentage: 95
        # targetMemoryUtilizationPercentage: 80
        customMetric:
          enabled: false
          name: hpa_custom_metrics_deployment_queues_length
          targetValue: 10
      gracePeriod: 3600
      resources:
        requests:
          memory: 400Mi
          cpu: 400m
        limits:
          memory: 1200Mi
          cpu: 2000m
      queues:
        - celery
        - machines
        - command
        - deployments
        - mappings
        - networks
        - volumes
    gevent:
      enabled: true
      replicas: 1
      autoscaling: *autoscaling
      pool: gevent
      resources:
        requests:
          memory: 150Mi
          cpu: 200m
        limits:
          memory: 250Mi
          cpu: 1000m
      gracePeriod: 3600
      queues:
        - scripts
        - probe
        - ping
        - rules

    # Queue-specific runners. Off by default.
    celery:
      enabled: false
      pool: prefork
      replicas: 1
      autoscaling: *autoscaling
      resources:
        requests:
          memory: 400Mi
          cpu: 500m
        limits:
          memory: 1000Mi
          cpu: 1000m
      queues:
        - celery
    machines:
      enabled: false
      pool: prefork
      replicas: 1
      autoscaling: *autoscaling
      resources:
        requests:
          memory: 400Mi
          cpu: 500m
        limits:
          memory: 1200Mi
          cpu: 2000m
      preemptible: true
      queues:
        - machines
    command:
      enabled: false
      pool: prefork
      replicas: 1
      autoscaling: *autoscaling
      resources:
        requests:
          memory: 300Mi
          cpu: 200m
        limits:
          memory: 800Mi
          cpu: 2000m
      queues:
        - command
    deployments:
      enabled: false
      pool: prefork
      replicas: 1
      autoscaling: *autoscaling
      resources:
        requests:
          memory: 300Mi
          cpu: 200m
        limits:
          memory: 800Mi
          cpu: 2000m
      gracePeriod: 3600
      queues:
        - deployments
    mappings:
      enabled: false
      pool: prefork
      replicas: 1
      autoscaling: *autoscaling
      resources:
        requests:
          memory: 300Mi
          cpu: 200m
        limits:
          memory: 800Mi
          cpu: 2000m
      queues:
        - mappings
    scripts:
      enabled: false
      pool: gevent
      replicas: 1
      autoscaling: *autoscaling
      resources:
        requests:
          memory: 200Mi
          cpu: 200m
        limits:
          memory: 1000Mi
          cpu: 2000m
      gracePeriod: 3600
      queues:
        - scripts
    probe:
      enabled: false
      pool: gevent
      replicas: 1
      autoscaling: *autoscaling
      resources:
        requests:
          memory: 100Mi
          cpu: 200m
        limits:
          memory: 200Mi
          cpu: 1000m
      preemptible: true
      queues:
        - probe
    ping:
      enabled: false
      pool: gevent
      replicas: 1
      autoscaling: *autoscaling
      resources:
        requests:
          memory: 100Mi
          cpu: 200m
        limits:
          memory: 200Mi
          cpu: 1000m
      preemptible: true
      queues:
        - ping
    rules:
      enabled: false
      pool: gevent
      replicas: 1
      autoscaling: *autoscaling
      resources:
        requests:
          memory: 100Mi
          cpu: 200m
        limits:
          memory: 400Mi
          cpu: 1000m
      preemptible: true
      queues:
        - rules
    networks:
      enabled: false
      pool: prefork
      replicas: 1
      autoscaling: *autoscaling
      resources:
        requests:
          memory: 400Mi
          cpu: 300m
        limits:
          memory: 1200Mi
          cpu: 2000m
      preemptible: true
      queues:
        - networks
    volumes:
      enabled: false
      pool: prefork
      replicas: 1
      autoscaling: *autoscaling
      resources:
        requests:
          memory: 400Mi
          cpu: 300m
        limits:
          memory: 1200Mi
          cpu: 2000m
      preemptible: true
      queues:
        - volumes
    zones:
      enabled: false
      pool: prefork
      replicas: 1
      autoscaling: *autoscaling
      resources:
        requests:
          memory: 400Mi
          cpu: 250m
        limits:
          memory: 1200Mi
          cpu: 2000m
      preemptible: true
      queues:
        - zones

  # Poller deployment settings.
  poller:
    generic:
      enabled: true
      replicas: 1
      autoscaling: *autoscaling
      resources:
        requests:
          memory: 400Mi
          cpu: 200m
        limits:
          memory: 700Mi
          cpu: 2000m
      preemptible: true
      scheduler: mist.api.poller.schedulers.PollingScheduler
    owners:
      enabled: false
      replicas: 1
      autoscaling: *autoscaling
      resources:
        requests:
          memory: 50Mi
          cpu: 300m
        limits:
          memory: 300Mi
          cpu: 1000m
      preemptible: true
      scheduler: mist.api.poller.schedulers.ShardedOwnerScheduler
    clouds:
      enabled: false
      replicas: 1
      autoscaling: *autoscaling
      resources:
        requests:
          memory: 200Mi
          cpu: 400m
        limits:
          memory: 500Mi
          cpu: 1000m
      preemptible: true
      scheduler: mist.api.poller.schedulers.ShardedCloudScheduler
    machines:
      enabled: false
      replicas: 1
      autoscaling: *autoscaling
      resources:
        requests:
          memory: 250Mi
          cpu: 500m
        limits:
          memory: 500Mi
          cpu: 1000m
      preemptible: true
      scheduler: mist.api.poller.schedulers.ShardedMachineScheduler

  # Cilia deployment settings. Set replicas to 0 to disable.
  cilia:
    replicas: 1
    resources:
      requests:
        memory: 150Mi
        cpu: 70m
      limits:
        memory: 300Mi
        cpu: 500m

  # Gocky deployment settings.
  gocky:
    replicas: 1
    autoscaling: *autoscaling
    resources:
      requests:
        memory: 10Mi
        cpu: 50m
      limits:
        memory: 50Mi
        cpu: 1000m

  # Api deployment settings.
  api:
    replicas: 1
    autoscaling: *autoscaling
    resources:
      requests:
        memory: 500Mi
        cpu: 300m
      limits:
        memory: 1000Mi
        cpu: 2000m

  # Dramatiq deployment settings.
  dramatiq:
    replicas: 1
    autoscaling: *autoscaling
    resources:
      requests:
        memory: 200Mi
        cpu: 300m
      limits:
        memory: 400Mi
        cpu: 1000m

  # Sockjs deployment settings.
  sockjs:
    replicas: 1
    autoscaling: *autoscaling
    resources:
      requests:
        memory: 100Mi
        cpu: 100m
      limits:
        memory: 300Mi
        cpu: 1000m

  # UI deployment settings.
  ui:
    replicas: 1
    autoscaling: *autoscaling
    resources:
      requests:
        memory: 50Mi
        cpu: 10m
      limits:
        memory: 250Mi
        cpu: 500m

  # Nginx deployment settings.
  nginx:
    replicas: 1
    autoscaling: *autoscaling
    resources:
      requests:
        memory: 30Mi
        cpu: 50m
      limits:
        memory: 60Mi
        cpu: 1000m

  # Landing deployment settings.
  landing:
    replicas: 1
    autoscaling: *autoscaling
    resources:
      requests:
        memory: 50Mi
        cpu: 10m
      limits:
        memory: 250Mi
        cpu: 200m
